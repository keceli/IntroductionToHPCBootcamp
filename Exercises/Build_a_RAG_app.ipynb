{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5630b0ca",
      "metadata": {
        "id": "5630b0ca"
      },
      "source": [
        "# Build a Retrieval Augmented Generation (RAG) App\n",
        "\n",
        "##### **NOTE:** *The tutorial content in this notebook are from LangChain tutorials. However, the implementation of the RAG pipeline in this notebook is not from LangChain, but follows a similar approach.*\n",
        "\n",
        "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or [RAG](/docs/concepts/rag/).\n",
        "\n",
        "LangChain offers a multi-part tutorial:\n",
        "\n",
        "- [Part 1](https://python.langchain.com/docs/tutorials/rag/) (similar to this guide) introduces RAG and walks through a minimal implementation.\n",
        "- [Part 2](https://python.langchain.com/docs/tutorials/qa_chat_history) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\n",
        "\n",
        "This tutorial will show how to build a simple Q&A application\n",
        "over a text data source. If you're already familiar with basic retrieval, you might also be interested in\n",
        "this [high-level overview of different retrieval techniques](https://python.langchain.com/docs/concepts/retrieval/).\n",
        "\n",
        "**Note**: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing [question/answering over SQL data](https://python.langchain.com/docs/tutorials/sql_qa/).\n",
        "\n",
        "## Overview\n",
        "A typical RAG application has two main components:\n",
        "\n",
        "**Indexing**: a pipeline for ingesting data from a source and indexing it. *This usually happens offline.*\n",
        "\n",
        "**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
        "\n",
        "Note: the indexing portion of this tutorial will largely follow the [semantic search tutorial](https://python.langchain.com/docs/tutorials/retrievers).\n",
        "\n",
        "The most common full sequence from raw data to answer looks like:\n",
        "\n",
        "### Indexing\n",
        "1. **Load**: First we need to load our data. This is done with [Document Loaders](https://python.langchain.com/docs/concepts/document_loaders).\n",
        "2. **Split**: [Text splitters](https://python.langchain.com/docs/concepts/text_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
        "3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://python.langchain.com/docs/concepts/vectorstores) and [Embeddings](https://python.langchain.com/docs/concepts/embedding_models) model.\n",
        "\n",
        "![index_diagram](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/rag_indexing.png?raw=1)\n",
        "\n",
        "### Retrieval and generation\n",
        "4. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](https://python.langchain.com/docs/concepts/retrievers).\n",
        "5. **Generate**: A [ChatModel](https://python.langchain.com/docs/concepts/chat_models) / [LLM](/docs/concepts/text_llms) produces an answer using a prompt that includes both the question with the retrieved data\n",
        "\n",
        "![retrieval_diagram](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/rag_retrieval_generation.png?raw=1)\n",
        "\n",
        "Once we've indexed our data, we will use [LangGraph](https://langchain-ai.github.io/langgraph/) as our orchestration framework to implement the retrieval and generation steps.\n",
        "\n",
        "## Setup\n",
        "\n",
        "### Jupyter Notebook\n",
        "\n",
        "This and other tutorials are perhaps most conveniently run in a [Jupyter notebooks](https://jupyter.org/). Going through guides in an interactive environment is a great way to better understand them. See [here](https://jupyter.org/install) for instructions on how to install.\n",
        "\n",
        "### Installation\n",
        "\n",
        "This tutorial requires these langchain dependencies:\n",
        "\n",
        "import Tabs from '@theme/Tabs';\n",
        "import TabItem from '@theme/TabItem';\n",
        "import CodeBlock from \"@theme/CodeBlock\";\n",
        "\n",
        "<Tabs>\n",
        "  <TabItem value=\"pip\" label=\"Pip\" default>\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1918ba2f",
      "metadata": {
        "id": "1918ba2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588a9b31-7344-4e1a-d6b6-1e8fa6cc166e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
        "%pip install --quiet --upgrade langchain-openai\n",
        "%pip install --quiet langchain_chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kyzvSYKnDoA",
        "outputId": "7e02eaa1-c36b-4699-d145-e24ae0d8b246"
      },
      "id": "3kyzvSYKnDoA",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa9ea6a-f914-4f50-8e35-52e6c34b8001",
      "metadata": {
        "id": "efa9ea6a-f914-4f50-8e35-52e6c34b8001"
      },
      "source": [
        "## Detailed walkthrough\n",
        "\n",
        "Let’s go through our code step-by-step to really understand what’s going on.\n",
        "\n",
        "## 1. Indexing {#indexing}\n",
        "\n",
        ":::note\n",
        "\n",
        "This section is an abbreviated version of the content in the [semantic search tutorial](/docs/tutorials/retrievers).\n",
        "If you're comfortable with [document loaders](/docs/concepts/document_loaders), [embeddings](/docs/concepts/embedding_models), and [vector stores](/docs/concepts/vectorstores),\n",
        "feel free to skip to the next section on [retrieval and generation](/docs/tutorials/rag/#orchestration).\n",
        "\n",
        ":::\n",
        "\n",
        "### 2. Loading documents\n",
        "\n",
        "We need to first load the blog post contents. We can use\n",
        "[DocumentLoaders](/docs/concepts/document_loaders)\n",
        "for this, which are objects that load in data from a source and return a\n",
        "list of\n",
        "[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n",
        "objects.\n",
        "\n",
        "In this case we’ll use the\n",
        "[WebBaseLoader](/docs/integrations/document_loaders/web_base),\n",
        "which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to\n",
        "parse it to text. We can customize the HTML -\\> text parsing by passing\n",
        "in parameters into the `BeautifulSoup` parser via `bs_kwargs` (see\n",
        "[BeautifulSoup\n",
        "docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)).\n",
        "In this case only HTML tags with class “post-content”, “post-title”, or\n",
        "“post-header” are relevant, so we’ll remove all others.\n",
        "\n",
        "### 3. Retrieve relevant documents\n",
        "Now that we have the vector database, we can use it to augment a query, serving as context information. The retriever will output the top-k relevant documents that are related to the query.\n",
        "\n",
        "### 4. Combine retrieved docs with query\n",
        "The retrieved document and query are both passed as input to the LLM. Basically, the LLM uses the documents to understand the context of the user query and rendrers answers. It's a good practice to compare results without using the RAG approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "26ef9d35",
      "metadata": {
        "id": "26ef9d35"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
      ],
      "metadata": {
        "id": "AyazTfAPttf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc358e8-1ef1-47a6-cc37-5f249137f06f"
      },
      "id": "AyazTfAPttf4",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "persistent_directory = os.path.join(\"db\", \"chroma_db\")\n",
        "\n",
        "# create the list of web contents to be used for RAG pipeline\n",
        "web_paths = [\"https://www.ibm.com/think/topics/hpc\",\n",
        "             \"https://www.geeksforgeeks.org/computer-organization-architecture/high-performance-computing/\"]\n",
        "\n",
        "# Check if the Chroma vector store already exists\n",
        "if not os.path.exists(persistent_directory):\n",
        "    print(\"Persistent directory does not exist. Initializing vector store...\")\n",
        "\n",
        "    # Load documents using LangChain WebBaseLoader\n",
        "    documents = []\n",
        "\n",
        "    # Read the text content from the file\n",
        "    for url in web_paths:\n",
        "        try:\n",
        "            # Load and chunk contents of the blog\n",
        "            loader = WebBaseLoader(url)\n",
        "            data = loader.load()\n",
        "            for doc in data:\n",
        "                # Add metadata to each document indicating its source\n",
        "                doc.metadata = {\"source\": url}\n",
        "                documents.append(doc)\n",
        "            print(f\"[✓] Loaded {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[✗] Failed to load {url}: {e}\")\n",
        "\n",
        "    for doc in documents:\n",
        "        doc.page_content = \" \".join(doc.page_content.split())  # remove white space\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Display information about the split documents\n",
        "    print(\"\\n--- Document Chunks Information ---\")\n",
        "    print(f\"Number of document chunks: {len(docs)}\")\n",
        "\n",
        "    # Create embeddings\n",
        "    print(\"\\n--- Creating embeddings ---\")\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model=\"text-embedding-3-small\"\n",
        "    )  # Update to a valid embedding model if needed\n",
        "    print(\"\\n--- Finished creating embeddings ---\")\n",
        "\n",
        "    # Create the vector store and persist it automatically\n",
        "    print(\"\\n--- Creating vector store ---\")\n",
        "    db = Chroma.from_documents(\n",
        "        docs, embeddings, persist_directory=persistent_directory)\n",
        "    print(\"\\n--- Finished creating vector store ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Vector store already exists. No need to initialize.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgf9RrT5tbaP",
        "outputId": "24b257d2-c45a-4661-a566-abb0b928f48c"
      },
      "id": "Xgf9RrT5tbaP",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persistent directory does not exist. Initializing vector store...\n",
            "[✓] Loaded https://www.ibm.com/think/topics/hpc\n",
            "[✓] Loaded https://www.geeksforgeeks.org/computer-organization-architecture/high-performance-computing/\n",
            "\n",
            "--- Document Chunks Information ---\n",
            "Number of document chunks: 63\n",
            "\n",
            "--- Creating embeddings ---\n",
            "\n",
            "--- Finished creating embeddings ---\n",
            "\n",
            "--- Creating vector store ---\n",
            "\n",
            "--- Finished creating vector store ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S62sBvQ-w0T8",
        "outputId": "f94eb46d-c880-48b3-fc1f-e27ddaf58710"
      },
      "id": "S62sBvQ-w0T8",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample chunk:\n",
            "What Is High-Performance Computing (HPC)? | IBM What is high-performance computing (HPC)? IT infrastructure 9 July 2024 Link copied Authors Stephanie Susnjara Author Ian Smalley Senior Editorial Strategist What is high-performance computing (HPC)? HPC is a technology that uses clusters of powerful processors that work in parallel to process massive, multidimensional data sets and solve complex problems at extremely high speeds. HPC solves some of today's most complex computing problems in real-time. HPC systems typically run at speeds more than one million times faster than the fastest commodity desktop, laptop or server systems. Supercomputers, purpose-built computers that embody millions of processors or processor cores, have been vital in high-performance computing for decades. Unlike mainframes, supercomputers are much faster and can run billions of floating-point operations in one second. Supercomputers are still with us; the fastest supercomputer is the US-based Frontier, with a\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total characters: {len(docs[0].page_content)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwdbc1BgxRE0",
        "outputId": "61ef8df1-227f-4436-e6f6-65f68c59eadb"
      },
      "id": "Bwdbc1BgxRE0",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "753e1484-e21b-4f62-9866-b3a5971f88a7",
      "metadata": {
        "id": "753e1484-e21b-4f62-9866-b3a5971f88a7",
        "outputId": "16838313-2f4f-4e59-d700-42ac687f1cf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your question: What is HPC?\n",
            "\n",
            "--- Relevant Documents ---\n",
            "Document 1:\n",
            "frequently used in HPC systems. It might be challenging for developers to learn how to create and optimise algorithms for parallel processing.Support for software and tools: To function effectively, HPC systems need specific software and tools. The options available to users may be constrained by the fact that not all software and tools are created to function with HPC equipment.Power consumption and cooling: To maintain the hardware functioning at its best, specialised cooling technologies are needed for HPC systems' high heat production. Furthermore, HPC systems consume a lot of electricity, which can be expensive and difficult to maintain.Applications of HPC High Performance Computing (HPC) is a term used to describe the use of supercomputers and parallel processing strategies to carry out difficult calculations and data analysis activities. From scientific research to engineering and industrial design, HPC is employed in a wide range of disciplines and applications. Here are a few\n",
            "\n",
            "Source: https://www.geeksforgeeks.org/computer-organization-architecture/high-performance-computing/\n",
            "\n",
            "Document 2:\n",
            "a foundation for scientific & industrial advancements.It is used in technologies like IoT, AI, 3D imaging evolves & amount of data that is used by organization is increasing exponentially to increase ability of a computer, we use High-performance computer.HPC is used to solve complex modeling problems in a spectrum of disciplines. It includes AI, Nuclear Physics, Climate Modelling, etc.HPC is applied to business uses, data warehouses & transaction processing. Need of High performance Computing : It will complete a time-consuming operation in less time.It will complete an operation under a light deadline and perform a high numbers of operations per second.It is fast computing, we can compute in parallel over lot of computation elements CPU, GPU, etc. It set up very fast network to connect between elements. Need of ever increasing Performance : Climate modelingDrug discoveryData AnalysisProtein foldingEnergy research How Does HPC Work? User/Scheduler → Compute cluster → Data storage To\n",
            "\n",
            "Source: https://www.geeksforgeeks.org/computer-organization-architecture/high-performance-computing/\n",
            "\n",
            "Document 3:\n",
            "What Is High-Performance Computing (HPC)? | IBM What is high-performance computing (HPC)? IT infrastructure 9 July 2024 Link copied Authors Stephanie Susnjara Author Ian Smalley Senior Editorial Strategist What is high-performance computing (HPC)? HPC is a technology that uses clusters of powerful processors that work in parallel to process massive, multidimensional data sets and solve complex problems at extremely high speeds. HPC solves some of today's most complex computing problems in real-time. HPC systems typically run at speeds more than one million times faster than the fastest commodity desktop, laptop or server systems. Supercomputers, purpose-built computers that embody millions of processors or processor cores, have been vital in high-performance computing for decades. Unlike mainframes, supercomputers are much faster and can run billions of floating-point operations in one second. Supercomputers are still with us; the fastest supercomputer is the US-based Frontier, with a\n",
            "\n",
            "Source: https://www.ibm.com/think/topics/hpc\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "query = input(\"Your question: \")\n",
        "\n",
        "# Retrieve relevant documents based on the query\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 3, \"score_threshold\": 0.3},\n",
        ")\n",
        "relevant_docs = retriever.invoke(query)\n",
        "\n",
        "# Display the relevant results with metadata\n",
        "print(\"\\n--- Relevant Documents ---\")\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "    if doc.metadata:\n",
        "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the query and the relevant document contents\n",
        "combined_input = (\n",
        "    \"Here are some documents that might help answer the question: \"\n",
        "    + query\n",
        "    + \"\\n\\nRelevant Documents:\\n\"\n",
        "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    + \"\\n\\nPlease provide a rough answer based only on the provided documents. If the answer is not found in the documents, respond with 'I'm not sure'.\"\n",
        ")\n",
        "\n",
        "# Create a ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# Define the messages for the model\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=combined_input),\n",
        "]\n",
        "\n",
        "# Invoke the model with the combined input\n",
        "result = llm.invoke(messages)\n",
        "\n",
        "# Display the full result and content only\n",
        "print(\"\\n--- Generated Response ---\")\n",
        "# print(\"Full result:\")\n",
        "# print(result)\n",
        "# print(result.content)\n",
        "\n",
        "# Display the content as markdown for better formatting\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "GIzGJmXp5KrQ",
        "outputId": "ea191b17-1610-430b-a405-e0d2cd7f5ca7"
      },
      "id": "GIzGJmXp5KrQ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generated Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "High-Performance Computing (HPC) refers to the use of supercomputers and parallel processing technologies to perform complex calculations and data analysis at extremely high speeds. HPC systems consist of clusters of powerful processors working in parallel to process massive and multidimensional data sets, solving difficult problems that require fast computation. These systems operate much faster than typical desktop or server computers, often running billions of operations per second. HPC is applied in various fields such as scientific research, engineering, AI, climate modeling, drug discovery, and energy research. It requires specialized hardware, software, and cooling technologies to function effectively."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversational chatBot without RAG\n",
        "\n",
        "This chatBot takes both input and previous conversation histrory to provide a response, thus helping to preserve the context of the conversation on the long-term."
      ],
      "metadata": {
        "id": "vr58GSYP6rjf"
      },
      "id": "vr58GSYP6rjf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a ChatOpenAI model\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "chat_history = []  # Use a list to store messages\n",
        "\n",
        "# Set an initial system message (optional)\n",
        "system_message = SystemMessage(content=\"You are a helpful assistant.\")\n",
        "chat_history.append(system_message)  # Add system message to chat history\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
        "\n",
        "    # Get AI response using history\n",
        "    result = llm.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
        "\n",
        "    # print(f\"ChatBot: {response}\")\n",
        "\n",
        "    print('\\n --- AI Generated Response ---\\n')\n",
        "\n",
        "    display(Markdown(response))\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "# print(\"---- Message History ----\")\n",
        "# print(chat_history)"
      ],
      "metadata": {
        "id": "JbVH75U55kwZ"
      },
      "id": "JbVH75U55kwZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tF2_xcIN63tK"
      },
      "id": "tF2_xcIN63tK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}