{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keceli/IntroductionToHPCBootcamp/blob/main/Exercises/Build_a_RAG_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5630b0ca",
      "metadata": {
        "id": "5630b0ca"
      },
      "source": [
        "# Build a Retrieval Augmented Generation (RAG) App\n",
        "\n",
        "##### **NOTE:** *The tutorial content in this notebook are from LangChain tutorials. However, the implementation of the RAG pipeline in this notebook is not from LangChain, but follows a similar approach.*\n",
        "\n",
        "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or [RAG](/docs/concepts/rag/).\n",
        "\n",
        "LangChain offers a multi-part tutorial:\n",
        "\n",
        "- [Part 1](https://python.langchain.com/docs/tutorials/rag/) (similar to this guide) introduces RAG and walks through a minimal implementation.\n",
        "- [Part 2](https://python.langchain.com/docs/tutorials/qa_chat_history) extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes.\n",
        "\n",
        "This tutorial will show how to build a simple Q&A application\n",
        "over a text data source. If you're already familiar with basic retrieval, you might also be interested in\n",
        "this [high-level overview of different retrieval techniques](https://python.langchain.com/docs/concepts/retrieval/).\n",
        "\n",
        "**Note**: Here we focus on Q&A for unstructured data. If you are interested for RAG over structured data, check out our tutorial on doing [question/answering over SQL data](https://python.langchain.com/docs/tutorials/sql_qa/).\n",
        "\n",
        "## Overview\n",
        "A typical RAG application has two main components:\n",
        "\n",
        "**Indexing**: a pipeline for ingesting data from a source and indexing it. *This usually happens offline.*\n",
        "\n",
        "**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
        "\n",
        "Note: the indexing portion of this tutorial will largely follow the [semantic search tutorial](https://python.langchain.com/docs/tutorials/retrievers).\n",
        "\n",
        "The most common full sequence from raw data to answer looks like:\n",
        "\n",
        "### Indexing\n",
        "1. **Load**: First we need to load our data. This is done with [Document Loaders](https://python.langchain.com/docs/concepts/document_loaders).\n",
        "2. **Split**: [Text splitters](https://python.langchain.com/docs/concepts/text_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
        "3. **Store**: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://python.langchain.com/docs/concepts/vectorstores) and [Embeddings](https://python.langchain.com/docs/concepts/embedding_models) model.\n",
        "\n",
        "![index_diagram](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/rag_indexing.png?raw=1)\n",
        "\n",
        "### Retrieval and generation\n",
        "4. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](https://python.langchain.com/docs/concepts/retrievers).\n",
        "5. **Generate**: A [ChatModel](https://python.langchain.com/docs/concepts/chat_models) / [LLM](/docs/concepts/text_llms) produces an answer using a prompt that includes both the question with the retrieved data\n",
        "\n",
        "![retrieval_diagram](https://github.com/langchain-ai/langchain/blob/master/docs/static/img/rag_retrieval_generation.png?raw=1)\n",
        "\n",
        "Once we've indexed our data, we will use [LangGraph](https://langchain-ai.github.io/langgraph/) as our orchestration framework to implement the retrieval and generation steps.\n",
        "\n",
        "## Setup\n",
        "\n",
        "### Jupyter Notebook\n",
        "\n",
        "This and other tutorials are perhaps most conveniently run in a [Jupyter notebooks](https://jupyter.org/). Going through guides in an interactive environment is a great way to better understand them. See [here](https://jupyter.org/install) for instructions on how to install.\n",
        "\n",
        "### Installation\n",
        "\n",
        "This tutorial requires these langchain dependencies:\n",
        "\n",
        "import Tabs from '@theme/Tabs';\n",
        "import TabItem from '@theme/TabItem';\n",
        "import CodeBlock from \"@theme/CodeBlock\";\n",
        "\n",
        "<Tabs>\n",
        "  <TabItem value=\"pip\" label=\"Pip\" default>\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1918ba2f",
      "metadata": {
        "id": "1918ba2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e342985e-6591-40c5-c5c0-cb59bdcf5fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.5/443.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.8/786.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
        "%pip install --quiet langchain-google-genai\n",
        "%pip install --quiet --upgrade langchain-openai\n",
        "%pip install --quiet langchain_chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kyzvSYKnDoA",
        "outputId": "55cdf85a-8bfb-49ed-dc4a-863ee9157ee0"
      },
      "id": "3kyzvSYKnDoA",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter API key for Google Gemini: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa9ea6a-f914-4f50-8e35-52e6c34b8001",
      "metadata": {
        "id": "efa9ea6a-f914-4f50-8e35-52e6c34b8001"
      },
      "source": [
        "## Detailed walkthrough\n",
        "\n",
        "Let’s go through our code step-by-step to really understand what’s going on.\n",
        "\n",
        "## 1. Indexing {#indexing}\n",
        "\n",
        ":::note\n",
        "\n",
        "This section is an abbreviated version of the content in the [semantic search tutorial](/docs/tutorials/retrievers).\n",
        "If you're comfortable with [document loaders](/docs/concepts/document_loaders), [embeddings](/docs/concepts/embedding_models), and [vector stores](/docs/concepts/vectorstores),\n",
        "feel free to skip to the next section on [retrieval and generation](/docs/tutorials/rag/#orchestration).\n",
        "\n",
        ":::\n",
        "\n",
        "### 2. Loading documents\n",
        "\n",
        "We need to first load the blog post contents. We can use\n",
        "[DocumentLoaders](/docs/concepts/document_loaders)\n",
        "for this, which are objects that load in data from a source and return a\n",
        "list of\n",
        "[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n",
        "objects.\n",
        "\n",
        "In this case we’ll use the\n",
        "[WebBaseLoader](/docs/integrations/document_loaders/web_base),\n",
        "which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to\n",
        "parse it to text. We can customize the HTML -\\> text parsing by passing\n",
        "in parameters into the `BeautifulSoup` parser via `bs_kwargs` (see\n",
        "[BeautifulSoup\n",
        "docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)).\n",
        "In this case only HTML tags with class “post-content”, “post-title”, or\n",
        "“post-header” are relevant, so we’ll remove all others.\n",
        "\n",
        "### 3. Retrieve relevant documents\n",
        "Now that we have the vector database, we can use it to augment a query, serving as context information. The retriever will output the top-k relevant documents that are related to the query.\n",
        "\n",
        "### 4. Combine retrieved docs with query\n",
        "The retrieved document and query are both passed as input to the LLM. Basically, the LLM uses the documents to understand the context of the user query and rendrers answers. It's a good practice to compare results without using the RAG approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26ef9d35",
      "metadata": {
        "id": "26ef9d35"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
      ],
      "metadata": {
        "id": "AyazTfAPttf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a589aaf7-483c-4015-ea6f-e83720914879"
      },
      "id": "AyazTfAPttf4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "persistent_directory = os.path.join(\"db\", \"chroma_db\")\n",
        "\n",
        "# create the list of web contents to be used for RAG pipeline\n",
        "web_paths = [\"https://www.ibm.com/think/topics/hpc\",\n",
        "             \"https://www.geeksforgeeks.org/computer-organization-architecture/high-performance-computing/\"]\n",
        "\n",
        "# Check if the Chroma vector store already exists\n",
        "if not os.path.exists(persistent_directory):\n",
        "    print(\"Persistent directory does not exist. Initializing vector store...\")\n",
        "\n",
        "    # Load documents using LangChain WebBaseLoader\n",
        "    documents = []\n",
        "\n",
        "    # Read the text content from the file\n",
        "    for url in web_paths:\n",
        "        try:\n",
        "            # Load and chunk contents of the blog\n",
        "            loader = WebBaseLoader(url)\n",
        "            data = loader.load()\n",
        "            for doc in data:\n",
        "                # Add metadata to each document indicating its source\n",
        "                doc.metadata = {\"source\": url}\n",
        "                documents.append(doc)\n",
        "            print(f\"[✓] Loaded {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[✗] Failed to load {url}: {e}\")\n",
        "\n",
        "    for doc in documents:\n",
        "        doc.page_content = \" \".join(doc.page_content.split())  # remove white space\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Display information about the split documents\n",
        "    print(\"\\n--- Document Chunks Information ---\")\n",
        "    print(f\"Number of document chunks: {len(docs)}\")\n",
        "\n",
        "    # Create embeddings\n",
        "    print(\"\\n--- Creating embeddings ---\")\n",
        "    # embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
        "    print(\"\\n--- Finished creating embeddings ---\")\n",
        "\n",
        "    # Create the vector store and persist it automatically\n",
        "    print(\"\\n--- Creating vector store ---\")\n",
        "    db = Chroma.from_documents(\n",
        "        docs, embeddings, persist_directory=persistent_directory)\n",
        "    print(\"\\n--- Finished creating vector store ---\")\n",
        "\n",
        "else:\n",
        "    print(\"Vector store already exists. No need to initialize.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgf9RrT5tbaP",
        "outputId": "fbeb6929-11c4-4671-9c7f-c5058ad0f07d"
      },
      "id": "Xgf9RrT5tbaP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Persistent directory does not exist. Initializing vector store...\n",
            "[✓] Loaded https://www.ibm.com/think/topics/hpc\n",
            "[✓] Loaded https://www.geeksforgeeks.org/computer-organization-architecture/high-performance-computing/\n",
            "\n",
            "--- Document Chunks Information ---\n",
            "Number of document chunks: 63\n",
            "\n",
            "--- Creating embeddings ---\n",
            "\n",
            "--- Finished creating embeddings ---\n",
            "\n",
            "--- Creating vector store ---\n",
            "\n",
            "--- Finished creating vector store ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Sample chunk:\\n{docs[0].page_content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S62sBvQ-w0T8",
        "outputId": "4082385f-ae22-4986-be71-c3552c7ad073"
      },
      "id": "S62sBvQ-w0T8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample chunk:\n",
            "What Is High-Performance Computing (HPC)? | IBM What is high-performance computing (HPC)? IT infrastructure 9 July 2024 Link copied Authors Stephanie Susnjara Author Ian Smalley Senior Editorial Strategist What is high-performance computing (HPC)? HPC is a technology that uses clusters of powerful processors that work in parallel to process massive, multidimensional data sets and solve complex problems at extremely high speeds. HPC solves some of today's most complex computing problems in real-time. HPC systems typically run at speeds more than one million times faster than the fastest commodity desktop, laptop or server systems. Supercomputers, purpose-built computers that embody millions of processors or processor cores, have been vital in high-performance computing for decades. Unlike mainframes, supercomputers are much faster and can run billions of floating-point operations in one second. Supercomputers are still with us; the fastest supercomputer is the US-based Frontier, with a\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total characters: {len(docs[0].page_content)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwdbc1BgxRE0",
        "outputId": "5f2c8f8f-3128-49e9-a163-c55480272354"
      },
      "id": "Bwdbc1BgxRE0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753e1484-e21b-4f62-9866-b3a5971f88a7",
      "metadata": {
        "id": "753e1484-e21b-4f62-9866-b3a5971f88a7",
        "outputId": "5d25f32f-2cde-4d32-d2ac-8aaad0588f58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your question: What is HPC?\n",
            "\n",
            "--- Relevant Documents ---\n",
            "Document 1:\n",
            "What Is High-Performance Computing (HPC)? | IBM What is high-performance computing (HPC)? IT infrastructure 9 July 2024 Link copied Authors Stephanie Susnjara Author Ian Smalley Senior Editorial Strategist What is high-performance computing (HPC)? HPC is a technology that uses clusters of powerful processors that work in parallel to process massive, multidimensional data sets and solve complex problems at extremely high speeds. HPC solves some of today's most complex computing problems in real-time. HPC systems typically run at speeds more than one million times faster than the fastest commodity desktop, laptop or server systems. Supercomputers, purpose-built computers that embody millions of processors or processor cores, have been vital in high-performance computing for decades. Unlike mainframes, supercomputers are much faster and can run billions of floating-point operations in one second. Supercomputers are still with us; the fastest supercomputer is the US-based Frontier, with a\n",
            "\n",
            "Source: https://www.ibm.com/think/topics/hpc\n",
            "\n",
            "Document 2:\n",
            "out difficult calculations and data analysis activities. From scientific research to engineering and industrial design, HPC is employed in a wide range of disciplines and applications. Here are a few of the most significant HPC use cases and applications: Scientific research: HPC is widely utilized in this sector, especially in areas like physics, chemistry, and astronomy. With standard computer techniques, it would be hard to model complex physical events, examine massive data sets, or carry out sophisticated calculations. Weather forecasting: The task of forecasting the weather is difficult and data-intensive, requiring sophisticated algorithms and a lot of computational power. Simulated weather models are executed on HPC computers to predict weather patterns. Healthcare: HPC is being used more and more in the medical field for activities like medication discovery, genome sequencing, and image analysis. Large volumes of medical data can be processed by HPC systems rapidly and\n",
            "\n",
            "Source: https://www.geeksforgeeks.org/computer-organization-architecture/high-performance-computing/\n",
            "\n",
            "Document 3:\n",
            "on numerous computer servers or processors. HPC uses massively parallel computing, which uses tens of thousands to millions of processors or processor cores. Computer clusters (also called HPC clusters) An HPC cluster comprises multiple high-speed computer servers networked with a centralized scheduler that manages the parallel computing workload. The computers, called nodes, use either high-performance multi-core CPUs or—more likely today—GPUs, which are well suited for rigorous mathematical calculations, machine learning (ML) models and graphics-intensive tasks. A single HPC cluster can include 100,000 or more nodes. Linux is the most widely used operating system for running HPC clusters. Other operating systems include Windows, Ubuntu and Unix. High-performance components All the other computing resources in an HPC cluster—such as networking, memory, storage and file systems—are high speed and high throughput. They are also low-latency components that can keep pace with the nodes\n",
            "\n",
            "Source: https://www.ibm.com/think/topics/hpc\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the existing vector store with the embedding function\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "query = input(\"Your question: \")\n",
        "\n",
        "# Retrieve relevant documents based on the query\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\",\n",
        "    search_kwargs={\"k\": 3, \"score_threshold\": 0.3},\n",
        ")\n",
        "relevant_docs = retriever.invoke(query)\n",
        "\n",
        "# Display the relevant results with metadata\n",
        "print(\"\\n--- Relevant Documents ---\")\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "    print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
        "    if doc.metadata:\n",
        "        print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the query and the relevant document contents\n",
        "combined_input = (\n",
        "    \"Here are some documents that might help answer the question: \"\n",
        "    + query\n",
        "    + \"\\n\\nRelevant Documents:\\n\"\n",
        "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    + \"\\n\\nPlease provide a rough answer based only on the provided documents. If the answer is not found in the documents, respond with 'I'm not sure'.\"\n",
        ")\n",
        "\n",
        "# Create an LLM model e.g., OpenAI GPT-4o or Google Gemini-2.5\n",
        "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "# Define the messages for the model\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "    HumanMessage(content=combined_input),\n",
        "]\n",
        "\n",
        "# Invoke the model with the combined input\n",
        "result = llm.invoke(messages)\n",
        "\n",
        "# Display the full result and content only\n",
        "print(\"\\n--- Generated Response ---\")\n",
        "# print(\"Full result:\")\n",
        "# print(result)\n",
        "# print(result.content)\n",
        "\n",
        "# Display the content as markdown for better formatting\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(result.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "GIzGJmXp5KrQ",
        "outputId": "84c4c02a-3620-438e-eebb-530bfd12544e"
      },
      "id": "GIzGJmXp5KrQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generated Response ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "HPC (High-Performance Computing) is a technology that utilizes clusters of powerful processors working in parallel to process massive, multidimensional data sets and solve complex problems at extremely high speeds. HPC systems typically run more than one million times faster than the fastest commodity desktop, laptop, or server systems.\n\nIt employs massively parallel computing, using tens of thousands to millions of processors or processor cores. HPC clusters consist of multiple high-speed computer servers networked with a centralized scheduler to manage the parallel computing workload. These computers, called nodes, often use high-performance multi-core CPUs or GPUs. All other computing resources within an HPC cluster, such as networking, memory, storage, and file systems, are also high-speed, high-throughput, and low-latency components."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversational chatBot without RAG\n",
        "\n",
        "This chatBot takes both input and previous conversation histrory to provide a response, thus helping to preserve the context of the conversation on the long-term."
      ],
      "metadata": {
        "id": "vr58GSYP6rjf"
      },
      "id": "vr58GSYP6rjf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an LLM model e.g., OpenAI GPT-4o or Google Gemini-2.5\n",
        "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "chat_history = []  # Use a list to store messages\n",
        "\n",
        "# Set an initial system message (optional)\n",
        "system_message = SystemMessage(content=\"You are a helpful assistant.\")\n",
        "chat_history.append(system_message)  # Add system message to chat history\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    query = input(\"You: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    chat_history.append(HumanMessage(content=query))  # Add user message\n",
        "\n",
        "    # Get AI response using history\n",
        "    result = llm.invoke(chat_history)\n",
        "    response = result.content\n",
        "    chat_history.append(AIMessage(content=response))  # Add AI message\n",
        "\n",
        "    # print(f\"ChatBot: {response}\")\n",
        "\n",
        "    print('\\n --- AI Generated Response ---\\n')\n",
        "\n",
        "    display(Markdown(response))\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "# print(\"---- Message History ----\")\n",
        "# print(chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JbVH75U55kwZ",
        "outputId": "7687a10d-d7ca-4948-ba32-1e0bc4d6156f"
      },
      "id": "JbVH75U55kwZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: What is ALCF?\n",
            "\n",
            " --- AI Generated Response ---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "ALCF stands for the **Argonne Leadership Computing Facility**.\n\nHere's a breakdown of what it is:\n\n1.  **Location and Affiliation:** It is a **U.S. Department of Energy (DOE) Office of Science User Facility** located at **Argonne National Laboratory** near Chicago, Illinois.\n\n2.  **Primary Mission:** Its core mission is to provide **leadership-class computing resources** to the scientific and engineering community. This means it hosts some of the world's most powerful supercomputers, designed to tackle the most challenging and complex scientific problems.\n\n3.  **User Facility:** As a user facility, researchers from academia, industry, and other national labs can apply for time on ALCF's systems to conduct large-scale, computationally intensive research. Access is typically granted through competitive peer-reviewed proposals.\n\n4.  **Purpose:** The goal is to accelerate scientific discovery and technological innovation by enabling simulations, data analysis, and AI applications that would be impossible on smaller computing systems. The research conducted at ALCF spans a wide range of scientific disciplines, including:\n    *   Materials science\n    *   Climate modeling\n    *   Astrophysics\n    *   Biology and medicine\n    *   Chemistry\n    *   High-energy physics\n    *   Engineering and design\n\n5.  **Key Systems:** ALCF has hosted and continues to host some of the most advanced supercomputers. Notable past systems include Mira, and current/future systems include Theta and the upcoming **Aurora** system, which is designed to be one of the world's first exascale supercomputers (capable of performing a quintillion operations per second).\n\nIn essence, ALCF is a critical national resource that empowers scientists to push the boundaries of knowledge using cutting-edge high-performance computing."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "You: Tell me about the Intro to HPC bootcamp.\n",
            "\n",
            " --- AI Generated Response ---\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The **ALCF Intro to HPC Bootcamp** is a specialized training program offered by the Argonne Leadership Computing Facility (ALCF) designed to introduce new users and researchers to the fundamentals of high-performance computing (HPC) and how to effectively utilize the ALCF's supercomputing resources.\n\nHere's a breakdown of what it typically involves:\n\n**1. Purpose and Goal:**\n*   To equip new users with the foundational knowledge and practical skills necessary to run scientific applications on leadership-class supercomputers like those at ALCF.\n*   To lower the barrier to entry for researchers who may have little to no prior experience with large-scale parallel computing.\n*   To optimize the use of ALCF's systems by teaching best practices for job submission, data management, and code optimization.\n\n**2. Target Audience:**\n*   Researchers, scientists, and engineers who are new to HPC.\n*   Graduate students and postdocs beginning to use supercomputers for their research.\n*   Existing ALCF users who want to refresh their skills or learn new techniques.\n*   Anyone interested in getting started with parallel programming and large-scale scientific computing.\n\n**3. Key Topics Typically Covered:**\nThe curriculum is intensive and hands-on, often covering:\n*   **Introduction to HPC Concepts:** What is HPC, parallel computing paradigms (MPI, OpenMP, CUDA), supercomputer architectures.\n*   **Linux Command Line Essentials:** Navigating the file system, common commands, shell scripting – crucial for interacting with supercomputers.\n*   **System Access and Usage:** How to log in, managing user environments, understanding file systems (home, project, scratch), software modules.\n*   **Job Submission and Resource Management:** Using job schedulers (e.g., Slurm) to submit, monitor, and manage computational jobs.\n*   **Compiling and Building Applications:** How to compile scientific codes on ALCF's specific systems, using different compilers and libraries.\n*   **Parallel Programming Basics:** Introduction to Message Passing Interface (MPI) for distributed memory parallelism and OpenMP for shared memory parallelism. (Note: These are often introductory, not deep dives into advanced topics).\n*   **Debugging and Profiling (Intro):** Basic tools and techniques for finding errors and identifying performance bottlenecks in parallel codes.\n*   **Data Management:** Efficiently moving data to and from the supercomputer, managing large datasets.\n*   **Best Practices:** Tips for efficient resource utilization, choosing the right algorithms, and optimizing code performance.\n\n**4. Format and Duration:**\n*   Bootcamps are typically **intensive**, lasting anywhere from a few days to a full week.\n*   They often combine **lectures** by ALCF staff and experts with extensive **hands-on exercises** where participants directly apply what they learn on ALCF's supercomputers.\n*   In recent years, many bootcamps have been conducted **virtually**, making them accessible to a wider audience, though historically they might have been in-person.\n\n**5. Prerequisites:**\n*   Participants are usually expected to have a basic understanding of a programming language (e.g., C, C++, Fortran, Python).\n*   Some familiarity with the Linux/Unix command line is beneficial, though a quick review is often provided.\n*   No prior HPC experience is generally required for the \"Intro\" bootcamp, as that's precisely what it aims to provide.\n\n**6. How to Participate:**\n*   ALCF typically announces these bootcamps on their official website (alcf.anl.gov) under their \"Events\" or \"Training\" sections.\n*   There's usually an **application process**, as spots can be limited and competitive. They look for participants who will benefit most from the training and are likely to use ALCF resources for their research.\n\nThe ALCF Intro to HPC Bootcamp is an excellent opportunity for researchers to gain the foundational skills needed to leverage the immense power of supercomputing for their scientific endeavors."
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "You: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tF2_xcIN63tK"
      },
      "id": "tF2_xcIN63tK",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
