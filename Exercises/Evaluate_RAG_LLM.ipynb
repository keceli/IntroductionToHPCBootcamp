{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "9b2bf2db1113463ebcd719832ed57d0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5181652f95d84e27a3a91034358e7f55",
       "IPY_MODEL_3ad67b2768f04631965ac4e28c5d0930",
       "IPY_MODEL_22a552b586fd47118af31f196763147e"
      ],
      "layout": "IPY_MODEL_ff6ea6133bb94a77b1463a7b19549bbc"
     }
    },
    "5181652f95d84e27a3a91034358e7f55": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc90197495d34069ac27fc7800e3a010",
      "placeholder": "​",
      "style": "IPY_MODEL_596a09c1b781418a91804815df06c0f4",
      "value": "Evaluating: 100%"
     }
    },
    "3ad67b2768f04631965ac4e28c5d0930": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ced772147f0a478e8d350b097f32e362",
      "max": 15,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f0a9ff6f572c4a83a96a562b0727815b",
      "value": 15
     }
    },
    "22a552b586fd47118af31f196763147e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fbdffc6b54a4f2689e913fb16748676",
      "placeholder": "​",
      "style": "IPY_MODEL_c5f7faf8877d4a53a9f15679e6021771",
      "value": " 15/15 [00:37&lt;00:00,  3.70s/it]"
     }
    },
    "ff6ea6133bb94a77b1463a7b19549bbc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc90197495d34069ac27fc7800e3a010": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "596a09c1b781418a91804815df06c0f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ced772147f0a478e8d350b097f32e362": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0a9ff6f572c4a83a96a562b0727815b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7fbdffc6b54a4f2689e913fb16748676": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5f7faf8877d4a53a9f15679e6021771": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The purpose of this Notebook is to illustrate a simple workflow for testing and evaluating LLM applications. We will also learn how to evalaute a RAG system using ragas. The evaluation framework uses the [ragas](https://docs.ragas.io/en/latest/) libary.\n",
    "\n",
    "**NOTE:** *The content in this notebook are mostly from the Ragas documentation and tutorial guide.*"
   ],
   "metadata": {
    "id": "peIZju-DWzFu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Installation**\n",
    "To get started, install Ragas and chat model(s) using pip with the following command:"
   ],
   "metadata": {
    "id": "ppbMKUf9Y3N4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%pip --quiet install ragas\n",
    "%pip install --quiet langchain-google-genai\n",
    "%pip install --quiet sacrebleu\n",
    "%pip install --quiet --upgrade langchain-openai"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7EzdIUwZSL5",
    "outputId": "c2ca21c9-1df3-4814-946b-59cf89aeb46e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/190.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m190.7/190.7 kB\u001B[0m \u001B[31m14.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/45.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.5/45.5 kB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/2.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m147.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m56.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m74.4/74.4 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m786.8/786.8 kB\u001B[0m \u001B[31m36.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.2/45.2 kB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m50.9/50.9 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.4/49.4 kB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.4/1.4 MB\u001B[0m \u001B[31m52.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m51.8/51.8 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m104.1/104.1 kB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **1. Evaluate Direct LLM**\n",
    "\n",
    "In the example below:\n",
    "\n",
    "\n",
    "\n",
    "*   A test sample containing user_input, response (the output from the LLM), and reference (the expected output from the LLM) as data points to evaluate the summary.\n",
    "*   The **BleuScore** metric is used to evaluate the quality of response by comparing it with reference. It measures the similarity between the response and the reference based on n-gram precision and brevity penalty. BLEU score ranges from 0 to 1, where 1 indicates a perfect match between the response and the reference.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Limitations with BlueScore**\n",
    "\n",
    "\n",
    "\n",
    "*   Time-Consuming Preparation: Evaluating the application requires preparing the expected output (reference) for each input, which can be both time-consuming and challenging.\n",
    "*   Inaccurate Scoring: Even though the response and reference are similar, the output score was low."
   ],
   "metadata": {
    "id": "6l2xtDavYpYV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "import bs4\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPLoDwAQbegp",
    "outputId": "d8c93ee3-0b7e-456d-c942-c09db4f418a6"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "from google.colab import userdata\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
   ],
   "metadata": {
    "id": "9k9StjKvAG7s"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "user_input = \"What is HPC?\"   # type in your input, e.g., a question\n",
    "reference = \"HPC (High-Performance Computing) is a technology that utilizes clusters of powerful processors working in parallel to process massive, multidimensional data sets and solve complex problems at extremely high speeds. HPC systems typically run more than one million times faster than the fastest commodity desktop, laptop, or server systems. It employs massively parallel computing, using tens of thousands to millions of processors or processor cores. HPC clusters consist of multiple high-speed computer servers networked with a centralized scheduler to manage the parallel computing workload. These computers, called nodes, often use high-performance multi-core CPUs or GPUs. All other computing resources within an HPC cluster, such as networking, memory, storage, and file systems, are also high-speed, high-throughput, and low-latency components.\"    # this is the ground-truth answer\n",
    "\n",
    "# Create an LLM model e.g., OpenAI GPT-4o or Google Gemini-2.5\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "result = llm.invoke(user_input)\n",
    "response = result.content\n",
    "\n",
    "print(f\"User Input: {user_input}\")\n",
    "print(f\"Reference: {reference}\")\n",
    "print(f\"LLM Response: {response}\")"
   ],
   "metadata": {
    "id": "e229ZAASblwg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a9359aee-88c9-412c-e53a-032cf2c7575b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "User Input: What is HPC?\n",
      "Reference: HPC (High-Performance Computing) is a technology that utilizes clusters of powerful processors working in parallel to process massive, multidimensional data sets and solve complex problems at extremely high speeds. HPC systems typically run more than one million times faster than the fastest commodity desktop, laptop, or server systems. It employs massively parallel computing, using tens of thousands to millions of processors or processor cores. HPC clusters consist of multiple high-speed computer servers networked with a centralized scheduler to manage the parallel computing workload. These computers, called nodes, often use high-performance multi-core CPUs or GPUs. All other computing resources within an HPC cluster, such as networking, memory, storage, and file systems, are also high-speed, high-throughput, and low-latency components.\n",
      "LLM Response: HPC stands for High-Performance Computing. It refers to the practice of using supercomputers and parallel processing techniques for running advanced application programs efficiently, reliably, and quickly. The main goal of HPC is to solve complex computational problems by leveraging aggregated computing power, which typically exceeds that of an average desktop computer.\n",
      "\n",
      "HPC systems are used in various fields, including:\n",
      "\n",
      "1. **Scientific Research**: To perform simulations and calculations in areas like quantum mechanics, climate research, and physics.\n",
      "2. **Engineering**: For designing and testing new products through simulations, such as automotive crash tests and aerodynamics modeling.\n",
      "3. **Data Analysis**: In fields like genomics, finance, and big data analytics, where the processing of large datasets quickly is crucial.\n",
      "4. **Artificial Intelligence and Machine Learning**: To train complex models more efficiently.\n",
      "\n",
      "HPC systems consist of many computing nodes that work together in parallel, usually connected with high-speed networks. They exploit modern technologies such as multicore processors, accelerators like GPUs, and fast interconnects to achieve high performance.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNgtyWI8WGwd",
    "outputId": "367df94f-c531-4b5b-d812-4241eaf28420"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.04773548444510098"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import BleuScore\n",
    "\n",
    "test_data = {\n",
    "    \"user_input\": user_input,\n",
    "    \"response\": response,\n",
    "    \"reference\": reference\n",
    "}\n",
    "metric = BleuScore()\n",
    "test_data = SingleTurnSample(**test_data)\n",
    "metric.single_turn_score(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **2. Evaluate RAG-assisted LLM**"
   ],
   "metadata": {
    "id": "G5lddXnWvmj2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, llm_model, embedding_model):\n",
    "        # self.llm = ChatGoogleGenerativeAI(model=llm_model)\n",
    "        # self.embeddings = GoogleGenerativeAIEmbeddings(model=embedding_model)\n",
    "        self.llm = ChatOpenAI(model=llm_model)\n",
    "        self.embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "        self.doc_embeddings = None\n",
    "        self.docs = None\n",
    "\n",
    "    def load_documents(self, documents):\n",
    "        \"\"\"Load documents and compute their embeddings.\"\"\"\n",
    "        self.docs = documents\n",
    "        self.doc_embeddings = self.embeddings.embed_documents(documents)\n",
    "\n",
    "    def get_most_relevant_docs(self, query):\n",
    "        \"\"\"Find the most relevant document for a given query.\"\"\"\n",
    "        if not self.docs or not self.doc_embeddings:\n",
    "            raise ValueError(\"Documents and their embeddings are not loaded.\")\n",
    "\n",
    "        query_embedding = self.embeddings.embed_query(query)\n",
    "        similarities = [\n",
    "            np.dot(query_embedding, doc_emb)\n",
    "            / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "            for doc_emb in self.doc_embeddings\n",
    "        ]\n",
    "        most_relevant_doc_index = np.argmax(similarities)\n",
    "        return [self.docs[most_relevant_doc_index]]\n",
    "\n",
    "    def generate_answer(self, query, relevant_doc):\n",
    "        \"\"\"Generate an answer for a given query based on the most relevant document.\"\"\"\n",
    "        prompt = f\"question: {query}\\n\\nDocuments: {relevant_doc}\"\n",
    "        messages = [\n",
    "            (\"system\", \"You are a helpful assistant that answers questions based on given documents only.\"),\n",
    "            (\"human\", prompt),\n",
    "        ]\n",
    "        ai_msg = self.llm.invoke(messages)\n",
    "        return ai_msg.content"
   ],
   "metadata": {
    "id": "UydoqGHUhVB0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# create the list of web contents to be used for RAG pipeline\n",
    "web_paths = [\"https://www.ibm.com/think/topics/hpc\",\n",
    "             \"https://www.geeksforgeeks.org/computer-organization-architecture/high-performance-computing/\"]\n",
    "\n",
    "# Load documents using LangChain WebBaseLoader\n",
    "documents = []\n",
    "\n",
    "# Read the text content from the file\n",
    "for url in web_paths:\n",
    "    try:\n",
    "        # Load and chunk contents of the blog\n",
    "        loader = WebBaseLoader(url)\n",
    "        data = loader.load()\n",
    "        for doc in data:\n",
    "            # Add metadata to each document indicating its source\n",
    "            doc.metadata = {\"source\": url}\n",
    "            documents.append(doc)\n",
    "        print(f\"[✓] Loaded {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[✗] Failed to load {url}: {e}\")\n",
    "\n",
    "for doc in documents:\n",
    "    doc.page_content = \" \".join(doc.page_content.split())  # remove white space\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "chunked_docs = [doc.page_content for doc in docs]"
   ],
   "metadata": {
    "id": "cY4d3HETf2Ik",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "be60e065-9f0c-447b-a67d-9769d3fb9418"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[✓] Loaded https://www.ibm.com/think/topics/hpc\n",
      "[✓] Loaded https://www.geeksforgeeks.org/computer-organization-architecture/high-performance-computing/\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# llm_model = \"gemini-2.5-flash\"\n",
    "# embedding_model = \"models/gemini-embedding-001\"\n",
    "llm_model = \"gpt-4o\"\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "\n",
    "# Initialize RAG instance\n",
    "rag = RAG(llm_model, embedding_model)\n",
    "\n",
    "# Load documents\n",
    "rag.load_documents(chunked_docs)"
   ],
   "metadata": {
    "id": "iShhEFC0f76j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Query and retrieve the most relevant document\n",
    "query = \"How do I submit a job to a supercomputer?\"\n",
    "relevant_doc = rag.get_most_relevant_docs(query)\n",
    "\n",
    "# Generate an answer\n",
    "answer = rag.generate_answer(query, relevant_doc)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Relevant Document: {relevant_doc}\")\n",
    "print(f\"Answer: {answer}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mmsUilbbLSYV",
    "outputId": "27a64f77-78eb-4785-a816-2651cc3cff2e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Query: How do I submit a job to a supercomputer?\n",
      "Relevant Document: ['supercomputers are much faster and can run billions of floating-point operations in one second. Supercomputers are still with us; the fastest supercomputer is the US-based Frontier, with a processing speed of 1.206 exaflops or quintillion floating point operations per second (flops).1 But today, more organizations are running HPC services on clusters of high-speed computer servers hosted on premises or in the cloud. HPC workloads uncover new insights that advance human knowledge and create significant competitive advantages. For example, HPC sequences DNA and automates stock trading. It runs artificial intelligence (AI) algorithms and simulations—like those enabling self-driving automobiles—that analyze terabytes of data streaming from IoT sensors, radar and GPS systems in real-time to make split-second decisions. Industry newsletter The latest tech news, backed by expert insights Stay up to date on the most important—and intriguing—industry trends on AI, automation, data and beyond']\n",
      "Answer: The document you provided does not contain information on the specific procedure for submitting a job to a supercomputer. To submit a job to a supercomputer, you would typically need to follow a set of instructions that can include:\n",
      "\n",
      "1. Access: Ensure you have access permissions to the supercomputer. This may require credentials or an account set up with the organization managing the supercomputer.\n",
      "\n",
      "2. Job Script: Write a job submission script, which is a text file that includes the commands to load necessary modules, run the program, and request resources like CPU time, memory, and nodes.\n",
      "\n",
      "3. Submit the Job: Use a job scheduler to submit your job script. Common job schedulers include SLURM, PBS, and LSF. You would use a command like `sbatch` for SLURM or `qsub` for PBS to submit the job script.\n",
      "\n",
      "4. Monitor: Once submitted, monitor the job’s status using commands provided by the scheduler (e.g., `squeue` for SLURM or `qstat` for PBS).\n",
      "\n",
      "5. Collect Results: After the job completes, retrieve the output files from the job execution for analysis.\n",
      "\n",
      "For detailed instructions, you should refer to the specific documentation or user guide provided by the supercomputer’s administration.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Collect and/or Load your Evaluation Data"
   ],
   "metadata": {
    "id": "uOIh_XITBqzu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sample_batch = [\n",
    "  {\n",
    "    \"question\": \"What is High-Performance Computing (HPC)?\",\n",
    "    \"answer\": \"High-Performance Computing (HPC) refers to the use of parallel processing by networking multiple computers (often clustered) to achieve performance far beyond that of typical desktops. These systems enable efficient and fast execution of advanced applications, particularly those reaching teraflop-scale computing — a trillion floating-point operations per second.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Why is HPC critically important?\",\n",
    "    \"answer\": \"HPC is important because it pushes scientific discovery, supports complex modeling (e.g., AI, climate modeling, nuclear physics), manages massive datasets in business analytics and IoT, and accelerates areas like drug discovery, 3D imaging, and genome sequencing.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What are the primary needs that drive the adoption of HPC?\",\n",
    "    \"answer\": \"Organizations adopt HPC to reduce runtime for lengthy computations, meet tight deadlines by achieving high operations per second, and use parallel CPU and GPU resources efficiently through fast inter-node networks.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How does an HPC system operate?\",\n",
    "    \"answer\": \"An HPC system operates with a scheduler that distributes jobs to compute clusters, where each node runs parts of the workload in parallel. Data storage provides fast I/O for computations. Synchronization across nodes, networking, and storage is critical to overall efficiency.\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What are the key challenges associated with HPC?\",\n",
    "    \"answer\": \"Challenges include high cost (hardware, software, energy, personnel), scalability issues, massive data management needs, complexity of parallel programming, software compatibility constraints, and significant power and cooling requirements.\"\n",
    "  }\n",
    "]"
   ],
   "metadata": {
    "id": "_j0VHEUPf9PQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = []\n",
    "\n",
    "for pair in sample_batch:\n",
    "  query = pair[\"question\"]\n",
    "  reference = pair[\"answer\"]\n",
    "\n",
    "  relevant_docs = rag.get_most_relevant_docs(query)\n",
    "  response = rag.generate_answer(query, relevant_docs)\n",
    "  dataset.append(\n",
    "      {\n",
    "          \"user_input\":query,\n",
    "          \"retrieved_contexts\":relevant_docs,\n",
    "          \"response\":response,\n",
    "          \"reference\":reference\n",
    "      }\n",
    "  )"
   ],
   "metadata": {
    "id": "2swhKe3TLAJ-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can evaluate our RAG system on the collected dataset using a set of commonly used RAG evaluation metrics. You may choose any model as evaluator LLM for evaluation.\n",
    "\n",
    "### Generation-focused metrics:\n",
    "\n",
    "1. **faithfulness**:\n",
    "Measures how factual the generated answer is with respect to the provided contexts.\n",
    "It evaluates if the answer contains information that isn't present in or contradicts the retrieved contexts.\n",
    "\n",
    "2. **answer_relevancy**:\n",
    "Assesses how well the generated answer addresses the user's query, regardless of factuality.\n",
    "It measures if the answer is on-topic and responsive to what was asked.\n",
    "\n",
    "### Retieval-focused metrics:\n",
    "\n",
    "1. **context_relevance**:\n",
    "Evaluates how relevant the retrieved contexts are to the user's query.\n",
    "This metric helps determine if the retrieval component is returning useful information.\n",
    "\n",
    "2. **context_recall**:\n",
    "Measures how completely the retrieved contexts cover the information needed to answer the query.\n",
    "When ground truth is available, it assesses if all necessary information was retrieved.\n",
    "\n",
    "3. **context_precision**:\n",
    "Evaluates how focused and precise the retrieved contexts are.\n",
    "It measures whether the contexts contain mostly relevant information without excessive irrelevant content."
   ],
   "metadata": {
    "id": "ni81KVkSCaLy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from ragas import EvaluationDataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(dataset)"
   ],
   "metadata": {
    "id": "EhF3IBpgB6FK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "\n",
    "# Create an LLM model e.g., OpenAI GPT-4o or Google Gemini-2.5\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "result = evaluate(dataset=evaluation_dataset,metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],llm=evaluator_llm)\n",
    "print(result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9b2bf2db1113463ebcd719832ed57d0b",
      "5181652f95d84e27a3a91034358e7f55",
      "3ad67b2768f04631965ac4e28c5d0930",
      "22a552b586fd47118af31f196763147e",
      "ff6ea6133bb94a77b1463a7b19549bbc",
      "fc90197495d34069ac27fc7800e3a010",
      "596a09c1b781418a91804815df06c0f4",
      "ced772147f0a478e8d350b097f32e362",
      "f0a9ff6f572c4a83a96a562b0727815b",
      "7fbdffc6b54a4f2689e913fb16748676",
      "c5f7faf8877d4a53a9f15679e6021771"
     ]
    },
    "id": "rSRSN-94COpa",
    "outputId": "53e9d09c-4475-4fba-82c8-6083e542d271"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b2bf2db1113463ebcd719832ed57d0b"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "print(result)",
   "metadata": {
    "id": "J10m2K-yUp9I"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
