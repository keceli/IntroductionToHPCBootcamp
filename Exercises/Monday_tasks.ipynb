{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYG4k7/deYf5Ol4E0YzTjj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keceli/IntroductionToHPCBootcamp/blob/main/Exercises/Monday_tasks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuVo7Ihmylkk"
      },
      "outputs": [],
      "source": [
        "# Not required for Colab, since these libraries are installed by default\n",
        "#!pip install torch transformers -q\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load a pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"  # You can change this to another model if you like\n",
        "model_name =  \"Qwen/Qwen3-0.6B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "print(f\"Tokenizer loaded: {tokenizer.name_or_path}\")\n",
        "print(f\"Model loaded: {model.config.model_type}\")\n",
        "print(f\"Pad token set: {tokenizer.pad_token}\")\n",
        "# Set the pad token for the tokenizer if it's not already set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Text to tokenize\n",
        "text = \"HPC means high-performance\"\n",
        "print(f\"Original text: '{text}'\")\n",
        "\n",
        "# Tokenize the text\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "print(f\"Token IDs: {input_ids}\")\n",
        "\n",
        "# Predict the next token\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    logits = outputs.logits[:, -1, :]  # Get the logits for the last token\n",
        "\n",
        "# Get the top 5 predicted tokens and their probabilities\n",
        "probabilities = torch.softmax(logits, dim=-1)\n",
        "top_k_probabilities, top_k_indices = torch.topk(probabilities, k=5)\n",
        "\n",
        "print(\"\\nNext Token Predictions:\")\n",
        "for i in range(top_k_indices.size(1)):\n",
        "    token_id = top_k_indices[0, i].item()\n",
        "    probability = top_k_probabilities[0, i].item()\n",
        "    predicted_token = tokenizer.decode([token_id])\n",
        "    print(f\"Token: '{predicted_token}', Probability: {probability:.4f}\")\n",
        "\n",
        "\n",
        "def generate_response(user_input):\n",
        "    \"\"\"\n",
        "    Generates a response from the chatbot model based on user input.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The user's input string.\n",
        "\n",
        "    Returns:\n",
        "        str: The chatbot's generated response.\n",
        "    \"\"\"\n",
        "    # Tokenize the user input\n",
        "    input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate a response using the model\n",
        "    # Use appropriate generation parameters\n",
        "    chat_history_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=1000,  # Set a reasonable max length for the response\n",
        "        pad_token_id=tokenizer.eos_token_id, # Use the end-of-sequence token for padding\n",
        "        no_repeat_ngram_size=3, # Avoid repeating n-grams to make the response more diverse\n",
        "        do_sample=True, # Enable sampling\n",
        "        top_k=50, # Consider top 50 tokens\n",
        "        top_p=0.95, # Use nucleus sampling\n",
        "        temperature=0.7, # Control randomness\n",
        "    )\n",
        "\n",
        "    # Decode the generated response\n",
        "    response = tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    return response\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "        print(\"ChatBot: Goodbye!\")\n",
        "        break\n",
        "    response = generate_response(user_input)\n",
        "    print(f\"ChatBot: {response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Not required for Colab, since these libraries are installed by default\n",
        "#!pip install openai -q\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# Make sure to set your OpenAI API key in your environment variables\n",
        "# or replace os.getenv('OPENAI_API_KEY') with your actual key.\n",
        "# In Colab, you can add it to the 'Secrets' tab on the left pane.\n",
        "# Name the secret 'OPENAI_API_KEY'.\n",
        "\n",
        "# Cola\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "def get_openai_response(prompt, model=\"gpt-3.5-turbo\"):\n",
        "  \"\"\"\n",
        "  Gets a response from the OpenAI API.\n",
        "\n",
        "  Args:\n",
        "    prompt (str): The prompt to send to the model.\n",
        "    model (str): The model to use (default is \"gpt-3.5-turbo\").\n",
        "\n",
        "  Returns:\n",
        "    str: The model's response.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "  except Exception as e:\n",
        "    return f\"An error occurred: {e}\"\n",
        "\n",
        "# Example usage:\n",
        "# print(get_openai_response(\"What is the capital of France?\"))\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "        print(\"ChatBot: Goodbye!\")\n",
        "        break\n",
        "    response = get_openai_response(user_input)\n",
        "    print(f\"ChatBot: {response}\")"
      ],
      "metadata": {
        "id": "3Rq4NqRc_b4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You need to install langchain langchain_community\n",
        "!pip install langchain langchain_community -q"
      ],
      "metadata": {
        "id": "V-tNu3FTQlAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!pip install langchain langchain_community -q\n",
        "\n",
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Make sure to set your OpenAI API key in your environment variables\n",
        "# or replace os.getenv('OPENAI_API_KEY') with your actual key.\n",
        "# In Colab, you can add it to the 'Secrets' tab on the left pane.\n",
        "# Name the secret 'OPENAI_API_KEY'.\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# Initialize the OpenAI model\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Initialize memory for the conversation\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Create a conversation chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True # Set to True to see the prompt and response\n",
        ")\n",
        "\n",
        "# Start the conversation loop\n",
        "print(\"Chatbot: Hi! How can I help you today?\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    response = conversation.predict(input=user_input)\n",
        "    print(f\"Chatbot: {response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlWYfY6R_7RB",
        "outputId": "044a2913-d0c2-41db-d229-6a0103cc3a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2020497128.py:17: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm = OpenAI(temperature=0)\n",
            "/tmp/ipython-input-2020497128.py:20: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory()\n",
            "/tmp/ipython-input-2020497128.py:23: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :class:`~langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
            "  conversation = ConversationChain(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Hi! How can I help you today?\n"
          ]
        }
      ]
    }
  ]
}
